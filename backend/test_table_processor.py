#!/usr/bin/env python3
"""
Task 3.3: í‘œ ë°ì´í„° ì²˜ë¦¬ ë° êµ¬ì¡°í™” ê³ ë„í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

# .env íŒŒì¼ ë¡œë“œ (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

import asyncio
import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(__file__))

from agents.table_processor import TableProcessorAgent
from agents.base import DocumentProcessingState, ProcessingStatus

def get_pdf_info(file_path: str) -> tuple:
    """PDF ê¸°ë³¸ ì •ë³´ ë°˜í™˜ (ì´ í˜ì´ì§€ ìˆ˜, íŒŒì¼ í¬ê¸°)"""
    try:
        import pdfplumber
        with pdfplumber.open(file_path) as pdf:
            total_pages = len(pdf.pages)
    except Exception:
        try:
            import PyPDF2
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
        except Exception:
            total_pages = 1
    
    # íŒŒì¼ í¬ê¸°
    try:
        size_bytes = os.path.getsize(file_path)
        if size_bytes < 1024:
            file_size = f"{size_bytes} bytes"
        elif size_bytes < 1024 * 1024:
            file_size = f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            file_size = f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            file_size = f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    except Exception:
        file_size = "ì•Œ ìˆ˜ ì—†ìŒ"
    
    return total_pages, file_size

def analyze_pages_detailed(file_path: str, total_pages: int) -> list:
    """ì „ì²´ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„"""
    page_analysis = []
    
    try:
        from services.pdfplumber_table_extractor import PDFPlumberTableExtractor
        extractor = PDFPlumberTableExtractor()
        
        for page_num in range(1, min(total_pages + 1, 6)):  # ìµœëŒ€ 5í˜ì´ì§€
            debug_info = extractor.debug_table_detection(file_path, page_num=page_num)
            
            if "error" not in debug_info:
                page_info = debug_info.get("page_info", {})
                text_info = debug_info.get("text_info", {})
                line_info = debug_info.get("line_info", {})
                table_info = debug_info.get("table_detection", {})
                
                page_summary = {
                    'page_num': page_num,
                    'width': page_info.get('width', 0),
                    'height': page_info.get('height', 0),
                    'chars': text_info.get('total_chars', 0),
                    'lines': line_info.get('total_lines', 0),
                    'h_lines': line_info.get('horizontal_lines', 0),
                    'v_lines': line_info.get('vertical_lines', 0),
                    'tables_detected': table_info.get('tables_found', 0),
                    'table_details': table_info.get('table_details', [])
                }
                page_analysis.append(page_summary)
            else:
                # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ
                page_analysis.append({
                    'page_num': page_num,
                    'width': 0, 'height': 0,
                    'chars': 0, 'lines': 0,
                    'h_lines': 0, 'v_lines': 0,
                    'tables_detected': 0,
                    'table_details': []
                })
    except Exception as e:
        print(f"âš ï¸ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        for page_num in range(1, min(total_pages + 1, 6)):
            page_analysis.append({
                'page_num': page_num,
                'width': 0, 'height': 0,
                'chars': 0, 'lines': 0,
                'h_lines': 0, 'v_lines': 0,
                'tables_detected': 0,
                'table_details': []
            })
    
    return page_analysis

class TableProcessingReport:
    """í‘œ ì²˜ë¦¬ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥"""
    
    def __init__(self, test_pdf_path: str):
        self.test_pdf_path = test_pdf_path
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.report_data = {
            "metadata": {
                "test_file": test_pdf_path,
                "timestamp": self.timestamp,
                "test_type": "table_processing_analysis"
            },
            "pdf_info": {},
            "page_analysis": [],
            "agent_settings": {},
            "processing_stats": {},
            "extraction_results": {},
            "quality_report": {},
            "failure_analysis": {}
        }
        self.console_output = []
    
    def log_console(self, message: str):
        """ì½˜ì†” ì¶œë ¥ì„ ë¡œê·¸ì— ì €ì¥"""
        self.console_output.append(message)
        print(message)
    
    def set_pdf_info(self, total_pages: int, file_size: str):
        """PDF ê¸°ë³¸ ì •ë³´ ì„¤ì •"""
        self.report_data["pdf_info"] = {
            "total_pages": total_pages,
            "file_size": file_size,
            "file_exists": os.path.exists(self.test_pdf_path)
        }
    
    def set_page_analysis(self, page_analysis: list):
        """í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ ì„¤ì •"""
        self.report_data["page_analysis"] = page_analysis
    
    def set_agent_settings(self, agent):
        """ì—ì´ì „íŠ¸ ì„¤ì • ì •ë³´ ì €ì¥"""
        self.report_data["agent_settings"] = {
            "quality_threshold": agent.quality_threshold,
            "advanced_service_active": bool(agent.table_service),
            "pdfplumber_extractor_active": bool(hasattr(agent, 'pdfplumber_extractor') and agent.pdfplumber_extractor),
            "camelot_available": hasattr(agent, '_extract_with_camelot_lattice'),
            "tabula_available": hasattr(agent, '_extract_with_tabula')
        }
    
    def set_processing_stats(self, processing_time: float, extracted_tables: list, table_chunks: list, stats: dict):
        """ì „ì²´ ì²˜ë¦¬ í†µê³„ ì„¤ì •"""
        self.report_data["processing_stats"] = {
            "processing_time_seconds": processing_time,
            "total_tables_extracted": len(extracted_tables),
            "high_quality_tables": stats.get('high_quality_tables', 0),
            "table_chunks_generated": len(table_chunks),
            "average_confidence": stats.get('average_confidence', 0),
            "extraction_methods_used": stats.get('extraction_methods', [])
        }
    
    def set_extraction_results(self, extracted_tables: list):
        """í˜ì´ì§€ë³„ í‘œ ì¶”ì¶œ ê²°ê³¼ ì„¤ì •"""
        page_table_count = {}
        detailed_results = []
        
        for i, table in enumerate(extracted_tables):
            page_num = table.get('page_number', 'Unknown')
            if page_num not in page_table_count:
                page_table_count[page_num] = 0
            page_table_count[page_num] += 1
            
            # í‘œ ìƒì„¸ ì •ë³´
            table_detail = {
                "table_index": i,
                "table_id": table.get('table_id', f'table_{i}'),
                "page_number": page_num,
                "shape": table.get('shape', (0, 0)),
                "confidence": table.get('confidence', 0),
                "extraction_method": table.get('extraction_method', 'unknown'),
                "has_preview_data": False
            }
            
            # í‘œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            df = table.get('dataframe')
            if df is not None and not df.empty:
                table_detail["has_preview_data"] = True
                table_detail["preview_rows"] = []
                for row_idx in range(min(3, len(df))):  # ìµœëŒ€ 3í–‰
                    row_data = df.iloc[row_idx].tolist()[:5]  # ìµœëŒ€ 5ì—´
                    table_detail["preview_rows"].append(row_data)
            
            detailed_results.append(table_detail)
        
        self.report_data["extraction_results"] = {
            "page_table_counts": page_table_count,
            "detailed_table_info": detailed_results,
            "total_pages_with_tables": len(page_table_count)
        }
    
    def set_quality_report(self, quality_report: dict):
        """í’ˆì§ˆ ë³´ê³ ì„œ ì„¤ì •"""
        self.report_data["quality_report"] = quality_report
    
    def set_failure_analysis(self, page_analysis: list, has_tables: bool):
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ì„¤ì •"""
        if not has_tables:
            total_lines = sum(p.get('lines', 0) for p in page_analysis)
            total_h_lines = sum(p.get('h_lines', 0) for p in page_analysis)
            total_v_lines = sum(p.get('v_lines', 0) for p in page_analysis)
            
            # ì›ì¸ ì¶”ì •
            estimated_cause = "unknown"
            if total_h_lines < 5 and total_v_lines < 5:
                estimated_cause = "text_only_document"
            elif total_lines > 100:
                estimated_cause = "complex_layout"
            else:
                estimated_cause = "non_standard_tables_or_scanned"
            
            self.report_data["failure_analysis"] = {
                "has_extraction_failure": True,
                "total_lines": total_lines,
                "horizontal_lines": total_h_lines,
                "vertical_lines": total_v_lines,
                "estimated_cause": estimated_cause,
                "recommended_solutions": [
                    "pdfplumber ê³ ê¸‰ ì„¤ì • ì¡°ì •",
                    "OCR ê¸°ë°˜ í‘œ ì¶”ì¶œ ì‹œë„",
                    "ìˆ˜ë™ í‘œ ì˜ì—­ ì§€ì •"
                ]
            }
        else:
            self.report_data["failure_analysis"] = {
                "has_extraction_failure": False
            }
    
    def save_reports(self):
        """ë³´ê³ ì„œë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""
        # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„±
        reports_dir = Path("reports/table_processing")
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        base_filename = f"table_processing_report_{self.timestamp}"
        
        # 1. JSON ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
        json_file = reports_dir / f"{base_filename}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë¡œ ë³€í™˜
            def json_serializable(obj):
                """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
                if isinstance(obj, dict):
                    return {k: json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [json_serializable(item) for item in obj]
                elif hasattr(obj, '__dict__'):
                    return json_serializable(obj.__dict__)
                elif str(type(obj)) in ['<class \'pandas._libs.missing.NAType\'>', '<class \'numpy.float64\'>', '<class \'numpy.int64\'>']:
                    return None if str(obj) in ['<NA>', 'nan', 'NaN'] else str(obj)
                elif obj is None or (hasattr(obj, '__ne__') and obj != obj):  # NaN ì²´í¬
                    return None
                else:
                    try:
                        json.dumps(obj)  # ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
                        return obj
                    except (TypeError, ValueError):
                        return str(obj)
            
            serializable_data = json_serializable(self.report_data)
            json.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        # 2. í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥
        txt_file = reports_dir / f"{base_filename}_summary.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("í‘œ ì²˜ë¦¬ ë° êµ¬ì¡°í™” ê³ ë„í™” í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ\n")
            f.write("=" * 80 + "\n\n")
            
            # ê¸°ë³¸ ì •ë³´
            f.write(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {self.test_pdf_path}\n")
            f.write(f"ğŸ• í…ŒìŠ¤íŠ¸ ì‹œê°„: {self.timestamp}\n\n")
            
            # PDF ì •ë³´
            pdf_info = self.report_data["pdf_info"]
            f.write(f"ğŸ“– PDF ê¸°ë³¸ ì •ë³´:\n")
            f.write(f"   - ì´ í˜ì´ì§€: {pdf_info.get('total_pages', 'N/A')}í˜ì´ì§€\n")
            f.write(f"   - íŒŒì¼ í¬ê¸°: {pdf_info.get('file_size', 'N/A')}\n\n")
            
            # ì—ì´ì „íŠ¸ ì„¤ì •
            agent_settings = self.report_data["agent_settings"]
            f.write(f"ğŸ”§ ì—ì´ì „íŠ¸ ì„¤ì •:\n")
            f.write(f"   - í’ˆì§ˆ ì„ê³„ê°’: {agent_settings.get('quality_threshold', 'N/A')}%\n")
            f.write(f"   - ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤: {'í™œì„±í™”' if agent_settings.get('advanced_service_active') else 'ë¹„í™œì„±í™”'}\n")
            f.write(f"   - pdfplumber ì¶”ì¶œê¸°: {'í™œì„±í™”' if agent_settings.get('pdfplumber_extractor_active') else 'ë¹„í™œì„±í™”'}\n")
            f.write(f"   - Camelot: {'ì‚¬ìš© ê°€ëŠ¥' if agent_settings.get('camelot_available') else 'ì‚¬ìš© ë¶ˆê°€'}\n")
            f.write(f"   - Tabula: {'ì‚¬ìš© ê°€ëŠ¥' if agent_settings.get('tabula_available') else 'ì‚¬ìš© ë¶ˆê°€'}\n\n")
            
            # ì²˜ë¦¬ í†µê³„
            processing_stats = self.report_data["processing_stats"]
            f.write(f"ğŸ“Š ì „ì²´ ì²˜ë¦¬ í†µê³„:\n")
            f.write(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_stats.get('processing_time_seconds', 0):.2f}ì´ˆ\n")
            f.write(f"   - ì´ ì¶”ì¶œ í‘œ: {processing_stats.get('total_tables_extracted', 0)}ê°œ\n")
            f.write(f"   - ê³ í’ˆì§ˆ í‘œ: {processing_stats.get('high_quality_tables', 0)}ê°œ\n")
            f.write(f"   - í‘œ ì²­í¬ ìƒì„±: {processing_stats.get('table_chunks_generated', 0)}ê°œ\n")
            f.write(f"   - í‰ê·  ì‹ ë¢°ë„: {processing_stats.get('average_confidence', 0):.1f}%\n")
            f.write(f"   - ì‚¬ìš©ëœ ë°©ë²•: {', '.join(processing_stats.get('extraction_methods_used', []))}\n\n")
            
            # ì¶”ì¶œ ê²°ê³¼
            extraction_results = self.report_data["extraction_results"]
            if extraction_results.get('total_pages_with_tables', 0) > 0:
                f.write(f"ğŸ“‹ í˜ì´ì§€ë³„ í‘œ ì¶”ì¶œ ê²°ê³¼:\n")
                page_counts = extraction_results.get('page_table_counts', {})
                for page_num in sorted(page_counts.keys()):
                    count = page_counts[page_num]
                    f.write(f"   í˜ì´ì§€ {page_num}: {count}ê°œ í‘œ ì¶”ì¶œ\n")
                
                f.write(f"\nğŸ“ í‘œ ìƒì„¸ ì •ë³´:\n")
                for table_detail in extraction_results.get('detailed_table_info', [])[:10]:  # ìµœëŒ€ 10ê°œ
                    shape = table_detail.get('shape', (0, 0))
                    f.write(f"   í‘œ {table_detail.get('table_index', 0)+1}: "
                           f"í˜ì´ì§€ {table_detail.get('page_number', 'N/A')}, "
                           f"{shape[0]}í–‰Ã—{shape[1]}ì—´, "
                           f"{table_detail.get('confidence', 0):.1f}% "
                           f"({table_detail.get('extraction_method', 'unknown')})\n")
            else:
                f.write(f"âŒ ì¶”ì¶œëœ í‘œê°€ ì—†ìŠµë‹ˆë‹¤\n")
                
                # ì‹¤íŒ¨ ë¶„ì„
                failure_analysis = self.report_data["failure_analysis"]
                if failure_analysis.get('has_extraction_failure'):
                    f.write(f"\nğŸ” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:\n")
                    f.write(f"   - ì „ì²´ ë¼ì¸ ìˆ˜: {failure_analysis.get('total_lines', 0)}ê°œ\n")
                    f.write(f"   - ìˆ˜í‰ ë¼ì¸: {failure_analysis.get('horizontal_lines', 0)}ê°œ\n")
                    f.write(f"   - ìˆ˜ì§ ë¼ì¸: {failure_analysis.get('vertical_lines', 0)}ê°œ\n")
                    f.write(f"   - ì¶”ì • ì›ì¸: {failure_analysis.get('estimated_cause', 'unknown')}\n")
                    f.write(f"   - ê¶Œì¥ í•´ê²°ì±…:\n")
                    for solution in failure_analysis.get('recommended_solutions', []):
                        f.write(f"     * {solution}\n")
            
            f.write(f"\n")
        
        # 3. ì½˜ì†” ì¶œë ¥ ë¡œê·¸ ì €ì¥
        console_file = reports_dir / f"{base_filename}_console.log"
        with open(console_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(self.console_output))
        
        return {
            "json_report": str(json_file),
            "summary_report": str(txt_file),
            "console_log": str(console_file)
        }

async def test_table_processing():
    """í‘œ ì²˜ë¦¬ ë° êµ¬ì¡°í™” ê³ ë„í™” í…ŒìŠ¤íŠ¸"""
    # í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
    test_pdf_path = "uploads/pdf/test_policy.pdf"
    
    # ë³´ê³ ì„œ ìƒì„±ê¸° ì´ˆê¸°í™”
    report = TableProcessingReport(test_pdf_path)
    
    report.log_console("=" * 60)
    report.log_console("Task 3.3: í‘œ ë°ì´í„° ì²˜ë¦¬ ë° êµ¬ì¡°í™” ê³ ë„í™” í…ŒìŠ¤íŠ¸")
    report.log_console("=" * 60)
    
    # TableProcessorAgent ì´ˆê¸°í™”
    agent = TableProcessorAgent(quality_threshold=30.0)
    
    # ì‹¤ì œ PDF íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸
    if not os.path.exists(test_pdf_path):
        report.log_console(f"âš ï¸ í…ŒìŠ¤íŠ¸ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_pdf_path}")
        report.log_console("ğŸ’¡ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        await test_table_service_features_only()
        return
    
    report.log_console(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_pdf_path}")
    
    # PDF ê¸°ë³¸ ì •ë³´ ë¶„ì„
    total_pages, file_size = get_pdf_info(test_pdf_path)
    report.set_pdf_info(total_pages, file_size)
    
    report.log_console(f"ğŸ“– PDF ê¸°ë³¸ ì •ë³´:")
    report.log_console(f"   - ì´ í˜ì´ì§€: {total_pages}í˜ì´ì§€")
    report.log_console(f"   - íŒŒì¼ í¬ê¸°: {file_size}")
    report.log_console("")
    
    # í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„
    report.log_console(f"ğŸ“Š í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„:")
    page_analysis = analyze_pages_detailed(test_pdf_path, total_pages)
    report.set_page_analysis(page_analysis)
    
    for page_info in page_analysis[:5]:  # ìµœëŒ€ 5í˜ì´ì§€ë§Œ
        page_num = page_info['page_num']
        report.log_console(f"   í˜ì´ì§€ {page_num}: "
              f"{page_info['chars']:,}ì, "
              f"{page_info['lines']}ë¼ì¸ "
              f"(H:{page_info['h_lines']}/V:{page_info['v_lines']}), "
              f"í‘œ {page_info['tables_detected']}ê°œ")
        
        # í‘œ ìƒì„¸ ì •ë³´
        for i, table_detail in enumerate(page_info['table_details']):
            rows = table_detail.get('rows', 0)
            cols = table_detail.get('cols', 0)
            bbox = table_detail.get('bbox', [])
            if bbox and len(bbox) == 4:
                report.log_console(f"      í‘œ {i+1}: {rows}í–‰Ã—{cols}ì—´, ìœ„ì¹˜({bbox[0]:.0f},{bbox[1]:.0f})")
            else:
                report.log_console(f"      í‘œ {i+1}: {rows}í–‰Ã—{cols}ì—´")
    
    if total_pages > 5:
        report.log_console(f"   ... (ë‚˜ë¨¸ì§€ {total_pages - 5}í˜ì´ì§€ ìƒëµ)")
    report.log_console("")
    
    # ì—ì´ì „íŠ¸ ì„¤ì • ì €ì¥
    report.set_agent_settings(agent)
    
    report.log_console(f"ğŸ”§ ì—ì´ì „íŠ¸ ì„¤ì •:")
    report.log_console(f"   - í’ˆì§ˆ ì„ê³„ê°’: {agent.quality_threshold}%")
    report.log_console(f"   - ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤: {'í™œì„±í™”' if agent.table_service else 'ë¹„í™œì„±í™”'}")
    report.log_console(f"   - pdfplumber ì¶”ì¶œê¸°: {'í™œì„±í™”' if hasattr(agent, 'pdfplumber_extractor') and agent.pdfplumber_extractor else 'ë¹„í™œì„±í™”'}")
    report.log_console(f"   - Camelot: {'ì‚¬ìš© ê°€ëŠ¥' if hasattr(agent, '_extract_with_camelot_lattice') else 'ì‚¬ìš© ë¶ˆê°€'}")
    report.log_console(f"   - Tabula: {'ì‚¬ìš© ê°€ëŠ¥' if hasattr(agent, '_extract_with_tabula') else 'ì‚¬ìš© ë¶ˆê°€'}")
    report.log_console("")
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì • (ì´ì „ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ ëª¨ë°©)
    state: DocumentProcessingState = {
        "file_path": test_pdf_path,
        "policy_id": "test_policy_001",
        "current_step": "table_extraction",
        "processed_pages": 0,
        "total_pages": total_pages,  # ì‹¤ì œ í˜ì´ì§€ ìˆ˜
        "extracted_text": [
            {
                "page_number": 1,
                "original_text": "ë‹¤ìŒ í‘œëŠ” ë³´í—˜ë£Œìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í‘œ 1ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.",
                "cleaned_text": "ë‹¤ìŒ í‘œëŠ” ë³´í—˜ë£Œìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í‘œ 1ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤."
            },
            {
                "page_number": 2,
                "original_text": "ìœ„ í‘œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, ë³´ìƒí•œë„ëŠ” ê°€ì…ê¸ˆì•¡ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤.",
                "cleaned_text": "ìœ„ í‘œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, ë³´ìƒí•œë„ëŠ” ê°€ì…ê¸ˆì•¡ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤."
            }
        ],
        "processed_chunks": [],
        "workflow_logs": []
    }
    
    # í‘œ ì¶”ì¶œ ë° êµ¬ì¡°í™” ì‹¤í–‰
    report.log_console("ğŸš€ í‘œ ë°ì´í„° ì²˜ë¦¬ ë° êµ¬ì¡°í™” ì‹œì‘...")
    start_time = time.time()
    
    try:
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result_state = await agent.process(state)
        
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„ (ìƒíƒœì™€ ì˜¤ë¥˜ ë©”ì‹œì§€ ëª¨ë‘ í™•ì¸)
        status = result_state.get("status")
        has_error = result_state.get("error_message") is not None
        
        if status == ProcessingStatus.COMPLETED.value and not has_error:
            report.log_console("âœ… í‘œ ì²˜ë¦¬ ë° êµ¬ì¡°í™” ì„±ê³µ!")
            
            # ì¶”ì¶œ ê²°ê³¼ í†µê³„
            extracted_tables = result_state.get("extracted_tables", [])
            table_chunks = [chunk for chunk in result_state.get("processed_chunks", []) 
                          if chunk['metadata']['chunk_type'].startswith('table')]
            stats = result_state.get("table_extraction_stats", {})
            
            # ì²˜ë¦¬ í†µê³„ ì €ì¥
            report.set_processing_stats(processing_time, extracted_tables, table_chunks, stats)
            
            report.log_console(f"ğŸ“Š ì „ì²´ ì²˜ë¦¬ í†µê³„:")
            report.log_console(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            report.log_console(f"   - ì´ ì¶”ì¶œ í‘œ: {len(extracted_tables)}ê°œ")
            report.log_console(f"   - ê³ í’ˆì§ˆ í‘œ: {stats.get('high_quality_tables', 0)}ê°œ")
            report.log_console(f"   - í‘œ ì²­í¬ ìƒì„±: {len(table_chunks)}ê°œ")
            report.log_console(f"   - í‰ê·  ì‹ ë¢°ë„: {stats.get('average_confidence', 0):.1f}%")
            report.log_console(f"   - ì‚¬ìš©ëœ ë°©ë²•: {', '.join(stats.get('extraction_methods', []))}")
            report.log_console("")
            
            # ì¶”ì¶œ ê²°ê³¼ ì €ì¥
            report.set_extraction_results(extracted_tables)
            
            if extracted_tables:
                # í˜ì´ì§€ë³„ í‘œ ì¶”ì¶œ ê²°ê³¼ ìƒì„¸ ë¶„ì„
                report.log_console(f"ğŸ“‹ í˜ì´ì§€ë³„ í‘œ ì¶”ì¶œ ê²°ê³¼:")
                page_table_count = {}
                
                for i, table in enumerate(extracted_tables):
                    page_num = table.get('page_number', 'Unknown')
                    if page_num not in page_table_count:
                        page_table_count[page_num] = []
                    page_table_count[page_num].append(table)
                
                for page_num in sorted(page_table_count.keys()):
                    page_tables = page_table_count[page_num]
                    print(f"   í˜ì´ì§€ {page_num}: {len(page_tables)}ê°œ í‘œ ì¶”ì¶œ")
                    
                    for j, table in enumerate(page_tables):
                        shape = table.get('shape', (0, 0))
                        confidence = table.get('confidence', 0)
                        method = table.get('extraction_method', 'unknown')
                        table_id = table.get('table_id', f'table_{j}')
                        
                        print(f"      í‘œ {j+1} ({table_id}): "
                              f"{shape[0]}í–‰Ã—{shape[1]}ì—´, "
                              f"ì‹ ë¢°ë„ {confidence:.1f}%, "
                              f"ë°©ë²•: {method}")
                        
                        # í‘œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²« 2í–‰)
                        df = table.get('dataframe')
                        if df is not None and not df.empty:
                            preview_row1 = df.iloc[0].tolist()[:3] if len(df) > 0 else ['N/A']
                            print(f"         ë¯¸ë¦¬ë³´ê¸°: {preview_row1}")
                            if len(df) > 1:
                                preview_row2 = df.iloc[1].tolist()[:3]
                                print(f"                   {preview_row2}")
                print()
            else:
                print("âŒ ì¶”ì¶œëœ í‘œê°€ ì—†ìŠµë‹ˆë‹¤")
                
                # í‘œê°€ ì—†ëŠ” ì´ìœ  ë¶„ì„
                print(f"\nğŸ” í‘œ ì¶”ì¶œ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")
                total_lines = sum(p['lines'] for p in page_analysis)
                total_h_lines = sum(p['h_lines'] for p in page_analysis)
                total_v_lines = sum(p['v_lines'] for p in page_analysis)
                
                print(f"   - ì „ì²´ ë¼ì¸ ìˆ˜: {total_lines}ê°œ (H:{total_h_lines}, V:{total_v_lines})")
                
                if total_h_lines < 5 and total_v_lines < 5:
                    print(f"   ğŸ’¡ ì¶”ì • ì›ì¸: ë¼ì¸ ê¸°ë°˜ í‘œê°€ ê±°ì˜ ì—†ìŒ (í…ìŠ¤íŠ¸ë§Œ êµ¬ì„±)")
                elif total_lines > 100:
                    print(f"   ğŸ’¡ ì¶”ì • ì›ì¸: ë³µì¡í•œ ë ˆì´ì•„ì›ƒìœ¼ë¡œ í‘œ ì¸ì‹ ì–´ë ¤ì›€")
                else:
                    print(f"   ğŸ’¡ ì¶”ì • ì›ì¸: í‘œ í˜•íƒœê°€ ì¼ë°˜ì ì´ì§€ ì•Šê±°ë‚˜ ìŠ¤ìº” ë¬¸ì„œ")
                
                print(f"   ğŸ”§ ê¶Œì¥ í•´ê²°ì±…:")
                print(f"      - pdfplumber ê³ ê¸‰ ì„¤ì • ì¡°ì •")
                print(f"      - OCR ê¸°ë°˜ í‘œ ì¶”ì¶œ ì‹œë„")
                print(f"      - ìˆ˜ë™ í‘œ ì˜ì—­ ì§€ì •")
                print()
            
            # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
            print("ğŸ“ˆ í’ˆì§ˆ ë¶„ì„ ì¤‘...")
            quality_report = agent.get_table_extraction_quality_report(result_state)
            
            if "error" not in quality_report:
                print("ğŸ“‹ í’ˆì§ˆ ë³´ê³ ì„œ:")
                summary = quality_report['extraction_summary']
                print(f"   - ì´ í‘œ ê°œìˆ˜: {summary['total_tables']}ê°œ")
                
                # í‘œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìƒì„¸ í’ˆì§ˆ ì •ë³´ ì¶œë ¥
                if summary['total_tables'] > 0:
                    print(f"   - ê³ í’ˆì§ˆ ë¹„ìœ¨: {summary['quality_ratio']:.1%}")
                    print(f"   - í‰ê·  ì‹ ë¢°ë„: {summary['average_confidence']:.1f}%")
                    
                    performance = quality_report['processing_performance']
                    print(f"   - ì²˜ë¦¬ ì†ë„: {performance['tables_per_second']:.1f} í‘œ/ì´ˆ")
                    
                    features = quality_report['advanced_features']
                    print(f"   - ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {'âœ…' if features.get('context_analysis') else 'âŒ'}")
                    print(f"   - ë³‘í•©ì…€ íƒì§€: {'âœ…' if features.get('merged_cell_detection') else 'âŒ'}")
                    print(f"   - í‘œ íƒ€ì… ë¶„ë¥˜: {'âœ…' if features.get('table_type_classification') else 'âŒ'}")
                else:
                    # í‘œê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ì¶œë ¥
                    if 'message' in summary:
                        print(f"   - ìƒíƒœ: {summary['message']}")
                
                features = quality_report['advanced_features']
                print(f"   - ê³ ê¸‰ ì„œë¹„ìŠ¤: {'âœ…' if features['advanced_service_used'] else 'âŒ'}")
                print(f"   - Java ì˜ì¡´ì„±: {'âŒ ë¬¸ì œ ìˆìŒ' if features.get('java_dependency_issue') else 'âœ… ì •ìƒ'}")
                print()
            
            # ìƒ˜í”Œ í‘œ ì¶œë ¥
            if extracted_tables:
                print("ğŸ“„ ìƒ˜í”Œ í‘œ (ì²˜ìŒ 2ê°œ):")
                for i, table in enumerate(extracted_tables[:2]):
                    print(f"   í‘œ {i+1}:")
                    print(f"     - ID: {table.get('table_id', 'Unknown')}")
                    print(f"     - í˜ì´ì§€: {table.get('page_number', 'Unknown')}")
                    print(f"     - ì¶”ì¶œë°©ë²•: {table.get('extraction_method', 'Unknown')}")
                    print(f"     - ì‹ ë¢°ë„: {table.get('confidence', 0):.1f}%")
                    print(f"     - í¬ê¸°: {table.get('shape', (0, 0))}")
                    print(f"     - íƒ€ì…: {table.get('table_type', 'unknown')}")
                    
                    # DataFrame ìƒ˜í”Œ ì¶œë ¥
                    df = table.get('dataframe')
                    if df is not None and not df.empty:
                        print(f"     - í—¤ë”: {list(df.columns)}")
                        print(f"     - ìƒ˜í”Œ ë°ì´í„°: {df.head(2).to_dict('records')}")
                    print()
            
            # ìƒ˜í”Œ ì²­í¬ ì¶œë ¥
            if table_chunks:
                print("ğŸ“„ ìƒ˜í”Œ í‘œ ì²­í¬ (ì²˜ìŒ 2ê°œ):")
                for i, chunk in enumerate(table_chunks[:2]):
                    print(f"   ì²­í¬ {i+1}:")
                    print(f"     - íƒ€ì…: {chunk['metadata']['chunk_type']}")
                    print(f"     - í˜ì´ì§€: {chunk['metadata']['page_number']}")
                    print(f"     - í‘œ ID: {chunk['metadata'].get('table_id', 'Unknown')}")
                    print(f"     - ê¸¸ì´: {len(chunk['text'])}ì")
                    print(f"     - ë‚´ìš©: {chunk['text'][:200]}...")
                    print()
            
            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            result_file = "test_table_extraction_result.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                # ê²°ê³¼ ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ì²˜ë¦¬
                serializable_result = {
                    "processing_time": processing_time,
                    "stats": stats,
                    "quality_report": quality_report,
                    "sample_tables": [
                        {
                            "table_id": table.get("table_id"),
                            "page_number": table.get("page_number"),
                            "extraction_method": table.get("extraction_method"),
                            "confidence": table.get("confidence"),
                            "shape": table.get("shape"),
                            "table_type": table.get("table_type"),
                            "headers": list(table["dataframe"].columns) if "dataframe" in table else [],
                            "sample_data": table["dataframe"].head(3).to_dict('records') if "dataframe" in table else []
                        }
                        for table in extracted_tables[:3]  # ì²˜ìŒ 3ê°œë§Œ
                    ],
                    "sample_chunks": [
                        {
                            "text": chunk["text"][:300] + "..." if len(chunk["text"]) > 300 else chunk["text"],
                            "metadata": chunk["metadata"]
                        }
                        for chunk in table_chunks[:3]  # ì²˜ìŒ 3ê°œë§Œ
                    ]
                }
                json.dump(serializable_result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result_file}")
            
        else:
            print("âŒ í‘œ ì²˜ë¦¬ ë° êµ¬ì¡°í™” ì‹¤íŒ¨!")
            print(f"   ìƒíƒœ: {result_state.get('status', 'Unknown')}")
            
            # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì¶”ì¶œ
            error_info = result_state.get('error_message', result_state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'))
            workflow_logs = result_state.get('workflow_logs', [])
            
            print(f"   ì˜¤ë¥˜: {error_info}")
            
            # ì›Œí¬í”Œë¡œìš° ë¡œê·¸ì—ì„œ ì˜¤ë¥˜ ì •ë³´ ì°¾ê¸°
            if workflow_logs:
                error_logs = [log for log in workflow_logs if log.get('level') == 'error']
                if error_logs:
                    print("   ìƒì„¸ ì˜¤ë¥˜ ë¡œê·¸:")
                    for log in error_logs[-3:]:  # ìµœê·¼ 3ê°œë§Œ
                        print(f"     - {log.get('message', 'No message')}")
            
            # í˜„ì¬ ë‹¨ê³„ ì •ë³´
            current_step = result_state.get('current_step', 'Unknown')
            print(f"   ì‹¤íŒ¨ ë‹¨ê³„: {current_step}")
    
        # ë³´ê³ ì„œ ì €ì¥
        report.log_console("\nğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ì¤‘...")
        try:
            saved_files = report.save_reports()
            report.log_console("âœ… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ!")
            report.log_console(f"   ğŸ“„ JSON ìƒì„¸ ë³´ê³ ì„œ: {saved_files['json_report']}")
            report.log_console(f"   ğŸ“‹ í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ: {saved_files['summary_report']}")
            report.log_console(f"   ğŸ“ ì½˜ì†” ë¡œê·¸: {saved_files['console_log']}")
        except Exception as save_error:
            report.log_console(f"âš ï¸ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {save_error}")
    
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë³´ê³ ì„œ ì €ì¥ ì‹œë„
        try:
            saved_files = report.save_reports()
            print(f"ğŸ“„ ì˜¤ë¥˜ ë³´ê³ ì„œ ì €ì¥: {saved_files['json_report']}")
        except:
            pass

async def test_table_service_features_only():
    """PDF ì—†ì´ í‘œ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (PDF ì—†ìŒ)")
    print("=" * 60)
    
    # TableProcessorAgent ì´ˆê¸°í™”
    agent = TableProcessorAgent()
    
    print(f"ğŸ¤– ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤: {'í™œì„±í™”' if agent.table_service else 'ë¹„í™œì„±í™”'}")
    print(f"ğŸ“Š Camelot: {'ì„¤ì¹˜ë¨' if hasattr(agent, '_extract_with_camelot_lattice') else 'ë¯¸ì„¤ì¹˜'}")
    print(f"ğŸ“‹ Tabula: {'ì„¤ì¹˜ë¨' if hasattr(agent, '_extract_with_tabula') else 'ë¯¸ì„¤ì¹˜'}")
    print(f"ğŸ¼ Pandas: {'ì„¤ì¹˜ë¨' if 'pandas' in sys.modules else 'ë¯¸ì„¤ì¹˜'}")
    
    if not agent.table_service:
        print("âŒ ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì§„í–‰")
        return
    
    # ê¸°ë³¸ ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸
    await test_table_service_features()

async def test_table_service_features():
    """í‘œ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê°œë³„ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê°œë³„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # TableProcessorAgent ì´ˆê¸°í™”
    agent = TableProcessorAgent()
    
    if not agent.table_service:
        print("âŒ ê³ ê¸‰ í‘œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ í‘œ ë°ì´í„°
    import pandas as pd
    
    sample_df = pd.DataFrame({
        'êµ¬ë¶„': ['Aíƒ€ì…', 'Bíƒ€ì…', 'Cíƒ€ì…'],
        'ë³´í—˜ë£Œìœ¨(%)': [1.5, 2.0, 2.5],
        'ë³´ìƒí•œë„(ë§Œì›)': [1000, 2000, 3000],
        'ë©´ì±…ê¸ˆì•¡(ë§Œì›)': [10, 20, 30]
    })
    
    sample_table = {
        'table_id': 'sample_001',
        'page_number': 1,
        'extraction_method': 'sample',
        'confidence': 85.0,
        'dataframe': sample_df,
        'shape': sample_df.shape
    }
    
    print("ğŸ“ ìƒ˜í”Œ í‘œ:")
    print(sample_df)
    print()
    
    # í‘œ êµ¬ì¡° ê°œì„  í…ŒìŠ¤íŠ¸
    print("ğŸ”§ í‘œ êµ¬ì¡° ê°œì„  í…ŒìŠ¤íŠ¸...")
    try:
        enhanced_table = agent.table_service._enhance_table_structure(sample_table)
        
        print(f"âœ… ê°œì„ ëœ í‘œ ì •ë³´:")
        print(f"   - í‘œ íƒ€ì…: {enhanced_table.get('table_type', 'unknown')}")
        print(f"   - ìº¡ì…˜: {enhanced_table.get('caption', 'N/A')}")
        print(f"   - í’ˆì§ˆ ì ìˆ˜: {enhanced_table.get('quality_score', 0):.1f}")
        print(f"   - ì»¬ëŸ¼ëª…: {enhanced_table.get('column_names', [])}")
        print()
        
        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ë³€í™˜ í…ŒìŠ¤íŠ¸
        print("ğŸ“„ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ë³€í™˜ í…ŒìŠ¤íŠ¸...")
        structured_text = agent.table_service.convert_table_to_structured_text(enhanced_table)
        
        print("âœ… ë³€í™˜ëœ í…ìŠ¤íŠ¸:")
        print(structured_text)
        
    except Exception as e:
        print(f"âŒ í‘œ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    await test_table_processing()
    await test_table_service_features()
    
    print("\n" + "=" * 60)
    print("Task 3.3 í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())
