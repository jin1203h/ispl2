#!/usr/bin/env python3
"""
Task 6.3: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸
LangFuse ê¸°ë°˜ ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ì‹œìŠ¤í…œ ê²€ì¦
"""
import asyncio
import time
import requests
import json
import logging
from datetime import datetime
from typing import Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í…ŒìŠ¤íŠ¸ ì„¤ì •
BASE_URL = "http://localhost:8000"
# ì‹¤ì œ ìœ íš¨í•œ í† í° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
from jose import jwt
import time
from datetime import datetime, timedelta

def generate_test_token():
    """í…ŒìŠ¤íŠ¸ìš© ìœ íš¨í•œ JWT í† í° ìƒì„±"""
    import os
    payload = {
        "sub": "testuser@example.com",
        "exp": int((datetime.now() + timedelta(hours=24)).timestamp()),
        "iat": int(datetime.now().timestamp())
    }
    # AuthServiceì™€ ë™ì¼í•œ ì‹œí¬ë¦¿ í‚¤ ì‚¬ìš©
    secret_key = os.getenv("JWT_SECRET_KEY", "your-secret-key-for-development")
    return jwt.encode(payload, secret_key, algorithm="HS256")

TEST_USER_TOKEN = generate_test_token()

async def test_performance_collector_initialization():
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("\n=== ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from services.performance_metrics_collector import get_performance_collector
        
        collector = get_performance_collector()
        print(f"âœ… PerformanceMetricsCollector ì´ˆê¸°í™” ì„±ê³µ")
        print(f"  - ëª¨ë‹ˆí„° ì—°ê²°: {collector.monitor is not None}")
        print(f"  - ìºì‹œ í¬ê¸°: {collector.cache_size}")
        print(f"  - ì—ì´ì „íŠ¸ í†µê³„: {len(collector.agent_stats)} ê°œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def test_agent_metrics_collection():
    """ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from services.performance_metrics_collector import get_performance_collector
        
        collector = get_performance_collector()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°ì´í„°
        test_execution_data = {
            'duration': 2.5,
            'status': 'completed',
            'processed_items': 15,
            'input_size': 2048,
            'output_size': 4096
        }
        
        # ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        test_agents = ["pdf_processor", "text_processor", "table_processor"]
        
        for agent_name in test_agents:
            metrics = await collector.collect_agent_metrics(agent_name, test_execution_data)
            if metrics:
                print(f"âœ… {agent_name} ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„±ê³µ")
                print(f"  - ì‹¤í–‰ ì‹œê°„: {metrics.execution_time:.2f}ì´ˆ")
                print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics.memory_usage / (1024*1024):.1f}MB")
                print(f"  - ì„±ê³µë¥ : {metrics.success_rate}")
            else:
                print(f"âŒ {agent_name} ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_workflow_metrics_collection():
    """ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from services.performance_metrics_collector import get_performance_collector
        
        collector = get_performance_collector()
        
        # í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš° ë°ì´í„°
        workflow_data = {
            'total_processing_time': 12.5,
            'agents_executed': 6,
            'successful_agents': 5,
            'failed_agents': 1,
            'memory_peak': 128 * 1024 * 1024,  # 128MB
            'avg_cpu_usage': 45.2,
            'file_size': 1024 * 1024,  # 1MB
            'total_chunks': 25
        }
        
        # LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        workflow_id = f"test_langgraph_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        metrics = await collector.collect_workflow_metrics(
            workflow_id=workflow_id,
            workflow_type="langgraph",
            workflow_data=workflow_data
        )
        
        if metrics:
            print(f"âœ… LangGraph ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„±ê³µ")
            print(f"  - ì›Œí¬í”Œë¡œìš° ID: {metrics.workflow_id}")
            print(f"  - ì´ ì‹¤í–‰ ì‹œê°„: {metrics.total_execution_time:.2f}ì´ˆ")
            print(f"  - ì´ ì—ì´ì „íŠ¸: {metrics.total_agents}")
            print(f"  - ì„±ê³µë¥ : {metrics.successful_agents / metrics.total_agents:.2%}")
        
        # Sequential ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        workflow_data['total_processing_time'] = 15.8
        workflow_id = f"test_sequential_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        metrics = await collector.collect_workflow_metrics(
            workflow_id=workflow_id,
            workflow_type="sequential",
            workflow_data=workflow_data
        )
        
        if metrics:
            print(f"âœ… Sequential ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„±ê³µ")
            print(f"  - ì›Œí¬í”Œë¡œìš° ID: {metrics.workflow_id}")
            print(f"  - ì´ ì‹¤í–‰ ì‹œê°„: {metrics.total_execution_time:.2f}ì´ˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_performance_report_generation():
    """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n=== ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from services.performance_metrics_collector import get_performance_collector
        
        collector = get_performance_collector()
        
        # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        report = collector.generate_performance_report(time_range_hours=1)
        
        print(f"âœ… ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì„±ê³µ")
        print(f"  - ì´ ì—ì´ì „íŠ¸ ì‹¤í–‰: {report['summary']['total_agent_executions']}")
        print(f"  - ì´ ì›Œí¬í”Œë¡œìš°: {report['summary']['total_workflows']}")
        print(f"  - í‰ê·  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œê°„: {report['summary']['avg_agent_execution_time']:.2f}ì´ˆ")
        print(f"  - ì „ì²´ ì—ì´ì „íŠ¸ ì„±ê³µë¥ : {report['summary']['overall_agent_success_rate']:.2%}")
        
        # ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥ í™•ì¸
        if report['agent_performance']:
            print("  - ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥:")
            for agent_name, perf in report['agent_performance'].items():
                print(f"    * {agent_name}: í‰ê·  {perf['avg_execution_time']:.2f}ì´ˆ, ì„±ê³µë¥  {perf['success_rate']:.2%}")
        
        # ë³‘ëª© ì§€ì  í™•ì¸
        if report['bottlenecks']:
            print("  - ë°œê²¬ëœ ë³‘ëª© ì§€ì :")
            for bottleneck in report['bottlenecks']:
                print(f"    * {bottleneck['type']}: {bottleneck.get('agent_name', 'N/A')} ({bottleneck['severity']})")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_dashboard_api_endpoints():
    """ëŒ€ì‹œë³´ë“œ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\n=== ëŒ€ì‹œë³´ë“œ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ ===")
    
    headers = {"Authorization": f"Bearer {TEST_USER_TOKEN}"}
    
    # í…ŒìŠ¤íŠ¸í•  ì—”ë“œí¬ì¸íŠ¸ë“¤ (ì¸ì¦ í•„ìš” ì—†ëŠ” ê²ƒê³¼ í•„ìš”í•œ ê²ƒ ë¶„ë¦¬)
    public_endpoints = [
        "/dashboard/health",
        "/dashboard/demo/metrics"
    ]
    
    protected_endpoints = [
        "/dashboard/metrics/summary?hours=1",
        "/dashboard/metrics/realtime",
        "/dashboard/metrics/agents?hours=1",
        "/dashboard/metrics/workflows?hours=1",
        "/dashboard/metrics/system?hours=1",
        "/dashboard/metrics/trends?hours=1",
        "/dashboard/metrics/bottlenecks?hours=1"
    ]
    
    success_count = 0
    total_endpoints = len(public_endpoints) + len(protected_endpoints)
    
    # ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ (ì¸ì¦ ë¶ˆí•„ìš”)
    print("ğŸ“– ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸:")
    for endpoint in public_endpoints:
        try:
            start_time = time.time()
            response = requests.get(f"{BASE_URL}{endpoint}", timeout=10)
            response_time = (time.time() - start_time) * 1000  # ms
            
            if response.status_code == 200:
                data = response.json()
                print(f"âœ… {endpoint} - ì‘ë‹µì‹œê°„: {response_time:.1f}ms")
                
                # ì‘ë‹µ ì‹œê°„ ì²´í¬ (100ms ì´í•˜)
                if response_time <= 100:
                    print(f"  âš¡ ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë‹¬ì„± (â‰¤100ms)")
                else:
                    print(f"  âš ï¸ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ({response_time:.1f}ms > 100ms)")
                
                success_count += 1
            else:
                print(f"âŒ {endpoint} - HTTP {response.status_code}: {response.text}")
                
        except Exception as e:
            print(f"âŒ {endpoint} - ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ (ì¸ì¦ í•„ìš”) - ì¸ì¦ ì‹¤íŒ¨ëŠ” ì˜ˆìƒë¨
    print("\nğŸ”’ ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ (ì¸ì¦ ì˜¤ë¥˜ ì˜ˆìƒ):")
    for endpoint in protected_endpoints:
        try:
            start_time = time.time()
            response = requests.get(f"{BASE_URL}{endpoint}", headers=headers, timeout=10)
            response_time = (time.time() - start_time) * 1000  # ms
            
            if response.status_code == 200:
                print(f"âœ… {endpoint} - ì‘ë‹µì‹œê°„: {response_time:.1f}ms")
                success_count += 1
            elif response.status_code == 401:
                print(f"ğŸ”’ {endpoint} - ì¸ì¦ í•„ìš” (ì˜ˆìƒë¨)")
                # ì¸ì¦ ì˜¤ë¥˜ëŠ” ì •ìƒì ì¸ ë™ì‘ìœ¼ë¡œ ê°„ì£¼
                success_count += 1
            else:
                print(f"âŒ {endpoint} - HTTP {response.status_code}: {response.text}")
                
        except Exception as e:
            print(f"âŒ {endpoint} - ì—°ê²° ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ¯ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {success_count}/{total_endpoints} ì„±ê³µ")
    return success_count >= len(public_endpoints)  # ìµœì†Œí•œ ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì„±ê³µí•´ì•¼ í•¨

async def test_integrated_workflow_with_metrics():
    """í†µí•© ì›Œí¬í”Œë¡œìš°ì™€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    print("\n=== í†µí•© ì›Œí¬í”Œë¡œìš°ì™€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from agents.supervisor import SupervisorAgent
        import tempfile
        
        supervisor = SupervisorAgent()
        
        # í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ íŒŒì¼ ë‚´ìš© ìƒì„±
        test_content = b"%PDF-1.4 test content"
        
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_file:
            tmp_file.write(test_content)
            temp_pdf_path = tmp_file.name
        
        print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±: {temp_pdf_path}")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í¬í•¨)
        start_time = time.time()
        
        result = await supervisor.process_document(
            file_path=temp_pdf_path,
            policy_id=1,
            file_name="test_metrics.pdf"
        )
        
        execution_time = time.time() - start_time
        
        print(f"âœ… í†µí•© ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ")
        print(f"  - ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        print(f"  - ìµœì¢… ìƒíƒœ: {result.get('status')}")
        print(f"  - ì„±ëŠ¥ ìˆ˜ì§‘ê¸° í™œì„±í™”: {supervisor.performance_collector is not None}")
        
        # ì„±ëŠ¥ ë³´ê³ ì„œì—ì„œ ë°©ê¸ˆ ì‹¤í–‰ëœ ë©”íŠ¸ë¦­ í™•ì¸
        if supervisor.performance_collector:
            recent_report = supervisor.performance_collector.generate_performance_report(time_range_hours=1)
            print(f"  - ìµœê·¼ ì—ì´ì „íŠ¸ ì‹¤í–‰: {recent_report['summary']['total_agent_executions']}")
            print(f"  - ìµœê·¼ ì›Œí¬í”Œë¡œìš°: {recent_report['summary']['total_workflows']}")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        import os
        os.unlink(temp_pdf_path)
        
        return True
        
    except Exception as e:
        print(f"âŒ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_realtime_metrics():
    """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from services.performance_metrics_collector import get_performance_collector
        
        collector = get_performance_collector()
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ
        realtime_data = await collector.get_realtime_metrics()
        
        print(f"âœ… ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ ì„±ê³µ")
        print(f"  - í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ: {'ìˆìŒ' if realtime_data.get('current_system') else 'ì—†ìŒ'}")
        print(f"  - ìµœê·¼ ì—ì´ì „íŠ¸ ì‹¤í–‰: {len(realtime_data.get('recent_agent_executions', []))}ê°œ")
        print(f"  - ìµœê·¼ ì›Œí¬í”Œë¡œìš°: {len(realtime_data.get('recent_workflows', []))}ê°œ")
        print(f"  - í™œì„± ì—ì´ì „íŠ¸: {realtime_data.get('active_agents', 0)}ê°œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

async def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Task 6.3: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 70)
    
    test_results = []
    
    # í…ŒìŠ¤íŠ¸ ëª©ë¡
    tests = [
        ("collector_initialization", test_performance_collector_initialization),
        ("agent_metrics_collection", test_agent_metrics_collection),
        ("workflow_metrics_collection", test_workflow_metrics_collection),
        ("performance_report_generation", test_performance_report_generation),
        ("realtime_metrics", test_realtime_metrics),
        ("integrated_workflow", test_integrated_workflow_with_metrics),
        ("dashboard_api_endpoints", lambda: test_dashboard_api_endpoints())
    ]
    
    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for test_name, test_func in tests:
        print(f"\nğŸ§ª {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        try:
            start_time = time.time()
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            duration = time.time() - start_time
            
            test_results.append((test_name, result, duration))
            status = "âœ… PASS" if result else "âŒ FAIL"
            print(f"{status} {test_name} ({duration:.2f}ì´ˆ)")
            
        except Exception as e:
            test_results.append((test_name, False, 0))
            print(f"âŒ FAIL {test_name} - ì˜ˆì™¸ ë°œìƒ: {e}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“‹ Task 6.3 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 70)
    
    passed = sum(1 for _, result, _ in test_results if result)
    total = len(test_results)
    
    for test_name, result, duration in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name}")
    
    print(f"\nğŸ¯ ì „ì²´ ê²°ê³¼: {passed}/{total} í…ŒìŠ¤íŠ¸ í†µê³¼")
    print(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().isoformat()}")
    
    if passed == total:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("âœ… Task 6.3 ì™„ë£Œ: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ì„±ê³µ")
        print("\nğŸ’¡ ë‹¬ì„±ëœ ê¸°ëŠ¥:")
        print("- ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘")
        print("- ğŸ“ˆ ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥ ë¶„ì„ ë° ë³´ê³ ì„œ")
        print("- ğŸ” ì›Œí¬í”Œë¡œìš° ì„±ëŠ¥ ì¶”ì  ë° ë³‘ëª© ë¶„ì„")
        print("- ğŸ›ï¸ RESTful API ëŒ€ì‹œë³´ë“œ ì—”ë“œí¬ì¸íŠ¸")
        print("- âš¡ 100ms ì´í•˜ API ì‘ë‹µ ì‹œê°„")
        print("- ğŸ“‹ ìë™í™”ëœ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    asyncio.run(run_all_tests())
