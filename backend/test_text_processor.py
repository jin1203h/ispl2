#!/usr/bin/env python3
"""
Task 3.2: í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ê°•í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

# .env íŒŒì¼ ë¡œë“œ (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

import asyncio
import sys
import os
import time
import json
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(__file__))

from agents.text_processor import TextProcessorAgent
from agents.base import DocumentProcessingState, ProcessingStatus

async def test_text_processing():
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ê°•í™” í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("Task 3.2: í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ê°•í™” í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # TextProcessorAgent ì´ˆê¸°í™”
    agent = TextProcessorAgent(chunk_size=200, chunk_overlap=40)
    
    # í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
    test_pdf_path = "test_policy.pdf"
    
    # ì‹¤ì œ PDF íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³ 
    if not os.path.exists(test_pdf_path):
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_pdf_path}")
        print("ğŸ’¡ ì‹¤ì œ ë³´í—˜ì•½ê´€ PDF íŒŒì¼ì„ 'test_policy.pdf'ë¡œ ì €ì¥í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.")
        return
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    state: DocumentProcessingState = {
        "file_path": test_pdf_path,
        "policy_id": "test_policy_001",
        "current_step": "text_extraction",
        "processed_pages": 0,
        "total_pages": 0,
        "processing_strategy": {
            "text_extraction": "pdfplumber",
            "ocr_required": True,  # OCR í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í™œì„±í™”
            "table_extraction": ["camelot_stream"],
            "image_processing": "basic",
            "optimization_level": "standard"
        },
        "workflow_logs": []
    }
    
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_pdf_path}")
    print(f"ğŸ”§ ì²­í‚¹ ì„¤ì •: {agent.chunk_size} í† í°, {agent.chunk_overlap} í† í° ì˜¤ë²„ë©")
    print(f"ğŸ¤– OCR ì„œë¹„ìŠ¤: {'í™œì„±í™”' if agent.ocr_service else 'ë¹„í™œì„±í™”'}")
    print(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ: {'í™œì„±í™”' if agent.text_cleaner else 'ë¹„í™œì„±í™”'}")
    print(f"ğŸ‡°ğŸ‡· í•œê¸€ ì²˜ë¦¬: {'í™œì„±í™”' if agent.korean_processor else 'ë¹„í™œì„±í™”'}")
    print()
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ì‹¤í–‰
    print("ğŸš€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ì‹œì‘...")
    start_time = time.time()
    
    try:
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result_state = await agent.process(state)
        
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        if result_state.get("status") == ProcessingStatus.COMPLETED:
            print("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ì„±ê³µ!")
            
            # ì¶”ì¶œ ê²°ê³¼ í†µê³„
            extracted_texts = result_state.get("extracted_text", [])
            processed_chunks = result_state.get("processed_chunks", [])
            stats = result_state.get("text_extraction_stats", {})
            
            print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"   - ì´ í˜ì´ì§€: {stats.get('total_pages', 0)}ê°œ")
            print(f"   - ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: {stats.get('total_text_length', 0):,}ì")
            print(f"   - í‰ê·  í˜ì´ì§€ë‹¹ í…ìŠ¤íŠ¸: {stats.get('average_text_per_page', 0):.0f}ì")
            print(f"   - ìƒì„±ëœ ì²­í¬: {len(processed_chunks)}ê°œ")
            print(f"   - OCR ì‚¬ìš©: {'ì˜ˆ' if stats.get('ocr_used', False) else 'ì•„ë‹ˆì˜¤'}")
            print()
            
            # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
            print("ğŸ“ˆ í’ˆì§ˆ ë¶„ì„ ì¤‘...")
            quality_report = agent.get_text_extraction_quality_report(result_state)
            
            if "error" not in quality_report:
                print("ğŸ“‹ í’ˆì§ˆ ë³´ê³ ì„œ:")
                print(f"   - ì¶”ì¶œ ë°©ë²•: {quality_report['extraction_method']}")
                print(f"   - ì •ì œ íš¨ìœ¨ì„±: {quality_report['text_statistics']['cleaning_efficiency']:.1%}")
                print(f"   - í‰ê·  ì²­í¬ í¬ê¸°: {quality_report['text_statistics']['average_chunk_size']:.0f}ì")
                print(f"   - í•œê¸€ í…ìŠ¤íŠ¸ ë¹„ìœ¨: {quality_report['quality_indicators']['korean_text_ratio']:.1f}%")
                print(f"   - ì²­í¬ ì¼ê´€ì„±: {quality_report['quality_indicators']['chunk_consistency']:.1f}%")
                print()
                
                # ê¸°ëŠ¥ ìƒíƒœ
                features = quality_report['processing_features']
                print("ğŸ”§ ê¸°ëŠ¥ ìƒíƒœ:")
                print(f"   - OCR í†µí•©: {'âœ…' if features['ocr_integration'] else 'âŒ'}")
                print(f"   - ë³´í—˜ì•½ê´€ ì •ì œ: {'âœ…' if features['insurance_text_cleaning'] else 'âŒ'}")
                print(f"   - í•œê¸€ ì²˜ë¦¬: {'âœ…' if features['korean_text_processing'] else 'âŒ'}")
                print(f"   - ì•½ê´€ êµ¬ì¡° íƒì§€: {'âœ…' if features['article_structure_detection'] else 'âŒ'}")
                print()
            
            # ìƒ˜í”Œ ì²­í¬ ì¶œë ¥
            if processed_chunks:
                print("ğŸ“„ ìƒ˜í”Œ ì²­í¬ (ì²˜ìŒ 3ê°œ):")
                for i, chunk in enumerate(processed_chunks[:3]):
                    print(f"   ì²­í¬ {i+1}:")
                    print(f"     - íƒ€ì…: {chunk['metadata']['chunk_type']}")
                    print(f"     - í˜ì´ì§€: {chunk['metadata']['page_number']}")
                    print(f"     - ê¸¸ì´: {len(chunk['text'])}ì")
                    print(f"     - ë‚´ìš©: {chunk['text'][:100]}...")
                    print()
            
            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            result_file = "test_text_extraction_result.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                # ê²°ê³¼ ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ì²˜ë¦¬
                serializable_result = {
                    "processing_time": processing_time,
                    "stats": stats,
                    "quality_report": quality_report,
                    "sample_chunks": [
                        {
                            "text": chunk["text"][:200] + "..." if len(chunk["text"]) > 200 else chunk["text"],
                            "metadata": chunk["metadata"]
                        }
                        for chunk in processed_chunks[:5]  # ì²˜ìŒ 5ê°œë§Œ
                    ]
                }
                json.dump(serializable_result, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result_file}")
            
        else:
            print("âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ ì‹¤íŒ¨!")
            print(f"   ì˜¤ë¥˜: {result_state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

async def test_text_cleaning_features():
    """í…ìŠ¤íŠ¸ ì •ì œ ê¸°ëŠ¥ ê°œë³„ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("í…ìŠ¤íŠ¸ ì •ì œ ê¸°ëŠ¥ ê°œë³„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # TextProcessorAgent ì´ˆê¸°í™”
    agent = TextProcessorAgent()
    
    if not agent.text_cleaner:
        print("âŒ í…ìŠ¤íŠ¸ ì •ì œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ìš© ì›ë³¸ í…ìŠ¤íŠ¸
    sample_text = """
    
    ë³´í—˜ì•½ê´€ì§‘     
    
    ì œ 1 ì¡° (ëª©ì )
    
    ì´ ì•½ê´€ì€    í”¼ë³´í—˜ì ë˜ëŠ” í”¼ë³´í—˜ì¸ì˜  ë³´í—˜ê¸ˆì•¡  ì— ê´€í•œ ì‚¬í•­ì„ ì •í•©ë‹ˆë‹¤.
    
    - 1 -
    
    ì œ2ì¡° ë³´ìƒí•œë„ì•¡
    
    ë³´í—˜ë£Œìœ¨    ì€   ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    
    í˜ì´ì§€ 1/10
    
    """
    
    print("ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸:")
    print(repr(sample_text))
    print()
    
    # í…ìŠ¤íŠ¸ ì •ì œ ì‹¤í–‰
    cleaned_text = agent._clean_extracted_text(sample_text)
    
    print("ğŸ§¹ ì •ì œëœ í…ìŠ¤íŠ¸:")
    print(repr(cleaned_text))
    print()
    
    # ì •ì œ í†µê³„
    if agent.text_cleaner:
        stats = agent.text_cleaner.get_cleaning_statistics(sample_text, cleaned_text)
        if stats:
            print("ğŸ“Š ì •ì œ í†µê³„:")
            print(f"   - ì›ë³¸ ê¸¸ì´: {stats['original_length']}ì")
            print(f"   - ì •ì œ í›„ ê¸¸ì´: {stats['cleaned_length']}ì")
            print(f"   - ê°ì†Œìœ¨: {stats['reduction_ratio']:.1%}")
            print(f"   - ì›ë³¸ ì¤„ ìˆ˜: {stats['original_lines']}")
            print(f"   - ì •ì œ í›„ ì¤„ ìˆ˜: {stats['cleaned_lines']}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    await test_text_processing()
    await test_text_cleaning_features()
    
    print("\n" + "=" * 60)
    print("Task 3.2 í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())


