"""
Task 3.6 í†µí•© PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ í†µí•© ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦
"""
import asyncio
import os
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

from dotenv import load_dotenv

from services.pdf_pipeline import (
    PDFProcessingPipeline, 
    PipelineConfig, 
    PipelineMode, 
    BatchProcessor
)
from agents.supervisor import SupervisorAgent
from utils.performance_monitor import ResourceOptimizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class IntegratedPipelineReport:
    """í†µí•© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ"""

    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.report_data = {
            "metadata": {
                "test_type": "integrated_pipeline_analysis",
                "timestamp": self.timestamp
            },
            "system_info": {},
            "pipeline_tests": {},
            "performance_analysis": {},
            "error_analysis": {},
            "recommendations": []
        }
        self.console_output = []

    def log_console(self, message: str):
        """ì½˜ì†” ì¶œë ¥ì„ ë¡œê·¸ì— ì €ì¥"""
        self.console_output.append(message)
        print(message)

    def set_system_info(self, system_status: Dict[str, Any]):
        """ì‹œìŠ¤í…œ ì •ë³´ ì„¤ì •"""
        self.report_data["system_info"] = system_status

    def add_pipeline_test(self, test_name: str, result: Dict[str, Any]):
        """íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€"""
        self.report_data["pipeline_tests"][test_name] = result

    def set_performance_analysis(self, analysis: Dict[str, Any]):
        """ì„±ëŠ¥ ë¶„ì„ ì„¤ì •"""
        self.report_data["performance_analysis"] = analysis

    def set_error_analysis(self, analysis: Dict[str, Any]):
        """ì—ëŸ¬ ë¶„ì„ ì„¤ì •"""
        self.report_data["error_analysis"] = analysis

    def add_recommendation(self, recommendation: str):
        """ê¶Œì¥ì‚¬í•­ ì¶”ê°€"""
        self.report_data["recommendations"].append(recommendation)

    def save_reports(self):
        """ë³´ê³ ì„œë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""
        reports_dir = Path("reports/integrated_pipeline")
        reports_dir.mkdir(parents=True, exist_ok=True)

        base_filename = f"integrated_pipeline_report_{self.timestamp}"

        # 1. JSON ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
        json_file = reports_dir / f"{base_filename}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            def json_serializable(obj):
                if hasattr(obj, 'value'):  # Enum ê°ì²´
                    return obj.value
                if isinstance(obj, dict):
                    return {k: json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [json_serializable(item) for item in obj]
                elif hasattr(obj, '__dict__'):
                    return json_serializable(obj.__dict__)
                elif obj is None or (hasattr(obj, '__ne__') and obj != obj):  # NaN ì²´í¬
                    return None
                else:
                    try:
                        json.dumps(obj)
                        return obj
                    except (TypeError, ValueError):
                        return str(obj)

            serializable_data = json_serializable(self.report_data)
            json.dump(serializable_data, f, indent=2, ensure_ascii=False)

        # 2. í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥
        txt_file = reports_dir / f"{base_filename}_summary.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("í†µí•© PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"ğŸ• í…ŒìŠ¤íŠ¸ ì‹œê°„: {self.timestamp}\n")
            f.write("\n")

            f.write("ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:\n")
            system_info = self.report_data["system_info"]
            if system_info:
                memory_info = system_info.get("memory", {})
                cpu_info = system_info.get("cpu", {})
                f.write(f"   - ì´ ë©”ëª¨ë¦¬: {memory_info.get('total_mb', 0):.1f} MB\n")
                f.write(f"   - ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {memory_info.get('available_mb', 0):.1f} MB\n")
                f.write(f"   - CPU ì‚¬ìš©ë¥ : {cpu_info.get('usage_percent', 0):.1f}%\n")
                f.write(f"   - CPU ì½”ì–´ ìˆ˜: {cpu_info.get('count', 0)}\n")
            f.write("\n")

            f.write("ğŸ§ª íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n")
            for test_name, result in self.report_data["pipeline_tests"].items():
                status = "âœ… ì„±ê³µ" if result.get("success", False) else "âŒ ì‹¤íŒ¨"
                f.write(f"   {status} {test_name}:\n")
                f.write(f"     - ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ\n")
                if result.get("error_message"):
                    f.write(f"     - ì˜¤ë¥˜: {result['error_message']}\n")
                f.write(f"     - ì™„ë£Œëœ ìŠ¤í…Œì´ì§€: {len(result.get('stages_completed', []))}\n")
            f.write("\n")

            f.write("ğŸ“Š ì„±ëŠ¥ ë¶„ì„:\n")
            perf_analysis = self.report_data["performance_analysis"]
            if perf_analysis:
                f.write(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {perf_analysis.get('avg_processing_time', 0):.2f}ì´ˆ\n")
                f.write(f"   - ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {perf_analysis.get('peak_memory_mb', 0):.1f} MB\n")
                f.write(f"   - ì „ì²´ ì„±ê³µë¥ : {perf_analysis.get('success_rate', 0):.1f}%\n")
                f.write(f"   - ì„±ëŠ¥ ë“±ê¸‰: {perf_analysis.get('performance_grade', 'N/A')}\n")
            f.write("\n")

            f.write("ğŸ” ê¶Œì¥ì‚¬í•­:\n")
            for rec in self.report_data["recommendations"]:
                f.write(f"   - {rec}\n")
            f.write("\n")

            f.write("=" * 80 + "\n")
            f.write("ì½˜ì†” ì¶œë ¥ ë¡œê·¸:\n")
            f.write("=" * 80 + "\n")
            for line in self.console_output:
                f.write(line + "\n")

        return {"json_report": str(json_file), "txt_summary": str(txt_file)}

async def test_standard_pipeline():
    """í‘œì¤€ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger.info("í‘œì¤€ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    config = PipelineConfig(
        mode=PipelineMode.STANDARD,
        parallel_processing=False,
        timeout_seconds=120
    )
    
    pipeline = PDFProcessingPipeline(config)
    test_file = "uploads/pdf/test_policy.pdf"
    
    # ë”ë¯¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(test_file):
        os.makedirs(os.path.dirname(test_file), exist_ok=True)
        with open(test_file, 'w') as f:
            f.write("dummy pdf content for testing")
    
    start_time = time.time()
    try:
        result = await pipeline.process_document(test_file, policy_id=1)
        processing_time = time.time() - start_time
        
        return {
            "success": result.success,
            "processing_time": processing_time,
            "stages_completed": result.stages_completed,
            "error_message": result.error_message,
            "performance_metrics": result.performance_metrics
        }
    except Exception as e:
        processing_time = time.time() - start_time
        return {
            "success": False,
            "processing_time": processing_time,
            "stages_completed": [],
            "error_message": str(e),
            "performance_metrics": {}
        }

async def test_fast_pipeline():
    """ê³ ì† íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ê³ ì† íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    config = PipelineConfig(
        mode=PipelineMode.FAST,
        enable_table_extraction=False,
        enable_image_extraction=False,
        parallel_processing=False,
        timeout_seconds=60
    )
    
    pipeline = PDFProcessingPipeline(config)
    test_file = "uploads/pdf/test_policy.pdf"
    
    start_time = time.time()
    try:
        result = await pipeline.process_document(test_file, policy_id=2)
        processing_time = time.time() - start_time
        
        return {
            "success": result.success,
            "processing_time": processing_time,
            "stages_completed": result.stages_completed,
            "error_message": result.error_message,
            "performance_metrics": result.performance_metrics
        }
    except Exception as e:
        processing_time = time.time() - start_time
        return {
            "success": False,
            "processing_time": processing_time,
            "stages_completed": [],
            "error_message": str(e),
            "performance_metrics": {}
        }

async def test_parallel_pipeline():
    """ë³‘ë ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ë³‘ë ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    config = PipelineConfig(
        mode=PipelineMode.STANDARD,
        parallel_processing=True,
        timeout_seconds=180
    )
    
    pipeline = PDFProcessingPipeline(config)
    test_file = "uploads/pdf/test_policy.pdf"
    
    start_time = time.time()
    try:
        result = await pipeline.process_document(test_file, policy_id=3)
        processing_time = time.time() - start_time
        
        return {
            "success": result.success,
            "processing_time": processing_time,
            "stages_completed": result.stages_completed,
            "error_message": result.error_message,
            "performance_metrics": result.performance_metrics
        }
    except Exception as e:
        processing_time = time.time() - start_time
        return {
            "success": False,
            "processing_time": processing_time,
            "stages_completed": [],
            "error_message": str(e),
            "performance_metrics": {}
        }

async def test_supervisor_integration():
    """SupervisorAgent í†µí•© í…ŒìŠ¤íŠ¸"""
    logger.info("SupervisorAgent í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    supervisor = SupervisorAgent()
    test_file = "uploads/pdf/test_policy.pdf"
    
    start_time = time.time()
    try:
        result = await supervisor.process_document_with_pipeline(
            test_file, 
            policy_id=4, 
            pipeline_mode="STANDARD"
        )
        processing_time = time.time() - start_time
        
        return {
            "success": result.get("status") != "failed",
            "processing_time": processing_time,
            "stages_completed": result.get("pipeline_result", {}).get("stages_completed", []),
            "error_message": result.get("error_message"),
            "performance_metrics": result.get("pipeline_result", {}).get("performance_metrics", {})
        }
    except Exception as e:
        processing_time = time.time() - start_time
        return {
            "success": False,
            "processing_time": processing_time,
            "stages_completed": [],
            "error_message": str(e),
            "performance_metrics": {}
        }

def analyze_performance(test_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
    """ì„±ëŠ¥ ë¶„ì„"""
    successful_tests = {k: v for k, v in test_results.items() if v["success"]}
    failed_tests = {k: v for k, v in test_results.items() if not v["success"]}
    
    if successful_tests:
        processing_times = [t["processing_time"] for t in successful_tests.values()]
        avg_processing_time = sum(processing_times) / len(processing_times)
        max_processing_time = max(processing_times)
        min_processing_time = min(processing_times)
    else:
        avg_processing_time = max_processing_time = min_processing_time = 0
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ (ì„±ëŠ¥ ë©”íŠ¸ë¦­ì—ì„œ ì¶”ì¶œ)
    peak_memory_mb = 0
    for test_result in successful_tests.values():
        metrics = test_result.get("performance_metrics", {})
        if isinstance(metrics, dict) and "peak_memory_mb" in metrics:
            peak_memory_mb = max(peak_memory_mb, metrics["peak_memory_mb"])
    
    success_rate = (len(successful_tests) / len(test_results)) * 100 if test_results else 0
    
    # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
    if success_rate >= 95 and avg_processing_time <= 30:
        performance_grade = "A (ìš°ìˆ˜)"
    elif success_rate >= 90 and avg_processing_time <= 60:
        performance_grade = "B (ì–‘í˜¸)"
    elif success_rate >= 80 and avg_processing_time <= 120:
        performance_grade = "C (ë³´í†µ)"
    else:
        performance_grade = "D (ê°œì„  í•„ìš”)"
    
    return {
        "total_tests": len(test_results),
        "successful_tests": len(successful_tests),
        "failed_tests": len(failed_tests),
        "success_rate": success_rate,
        "avg_processing_time": avg_processing_time,
        "max_processing_time": max_processing_time,
        "min_processing_time": min_processing_time,
        "peak_memory_mb": peak_memory_mb,
        "performance_grade": performance_grade
    }

def generate_recommendations(system_status: Dict[str, Any], 
                           performance_analysis: Dict[str, Any], 
                           test_results: Dict[str, Dict[str, Any]]) -> List[str]:
    """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¶Œì¥ì‚¬í•­
    memory_usage = system_status.get("memory", {}).get("used_percent", 0)
    if memory_usage > 80:
        recommendations.append("ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì¦ì„¤ì„ ê³ ë ¤í•˜ì„¸ìš”.")
    
    # ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­
    success_rate = performance_analysis.get("success_rate", 0)
    if success_rate < 90:
        recommendations.append("íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ì„ ê°•í™”í•˜ì„¸ìš”.")
    
    avg_time = performance_analysis.get("avg_processing_time", 0)
    if avg_time > 60:
        recommendations.append("í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ ê¹ë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
    
    # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ë¶„ì„
    failed_tests = [k for k, v in test_results.items() if not v["success"]]
    if failed_tests:
        recommendations.append(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {', '.join(failed_tests)}. í•´ë‹¹ ê¸°ëŠ¥ë“¤ì„ ì ê²€í•˜ì„¸ìš”.")
    
    # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
    if not recommendations:
        recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")
    
    return recommendations

async def run_integrated_pipeline_tests():
    """í†µí•© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("Task 3.6: í†µí•© PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)

    report = IntegratedPipelineReport()
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    system_status = ResourceOptimizer.get_system_status()
    report.set_system_info(system_status)
    report.log_console(f"ğŸ’» ì‹œìŠ¤í…œ ìƒíƒœ: ë©”ëª¨ë¦¬ {system_status['memory']['used_percent']:.1f}% ì‚¬ìš© ì¤‘")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_functions = {
        "standard_pipeline": test_standard_pipeline,
        "fast_pipeline": test_fast_pipeline,
        "parallel_pipeline": test_parallel_pipeline,
        "supervisor_integration": test_supervisor_integration
    }

    test_results = {}
    
    for test_name, test_func in test_functions.items():
        report.log_console(f"\nğŸ§ª {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        try:
            result = await test_func()
            test_results[test_name] = result
            status = "âœ… ì„±ê³µ" if result["success"] else "âŒ ì‹¤íŒ¨"
            report.log_console(f"{status} {test_name}: {result['processing_time']:.2f}ì´ˆ")
            if result.get("error_message"):
                report.log_console(f"   ì˜¤ë¥˜: {result['error_message']}")
        except Exception as e:
            test_results[test_name] = {
                "success": False,
                "processing_time": 0,
                "stages_completed": [],
                "error_message": str(e),
                "performance_metrics": {}
            }
            report.log_console(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì¶”ê°€
    for test_name, result in test_results.items():
        report.add_pipeline_test(test_name, result)

    # ì„±ëŠ¥ ë¶„ì„
    performance_analysis = analyze_performance(test_results)
    report.set_performance_analysis(performance_analysis)

    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    recommendations = generate_recommendations(system_status, performance_analysis, test_results)
    for rec in recommendations:
        report.add_recommendation(rec)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    report.log_console("\n" + "=" * 60)
    report.log_console("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    report.log_console("=" * 60)
    report.log_console(f"ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸: {performance_analysis['total_tests']}ê°œ")
    report.log_console(f"âœ… ì„±ê³µ: {performance_analysis['successful_tests']}ê°œ")
    report.log_console(f"âŒ ì‹¤íŒ¨: {performance_analysis['failed_tests']}ê°œ")
    report.log_console(f"ğŸ“ˆ ì„±ê³µë¥ : {performance_analysis['success_rate']:.1f}%")
    report.log_console(f"â±ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {performance_analysis['avg_processing_time']:.2f}ì´ˆ")
    report.log_console(f"ğŸ† ì„±ëŠ¥ ë“±ê¸‰: {performance_analysis['performance_grade']}")

    # ë³´ê³ ì„œ ì €ì¥
    report.log_console("\nğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ì¤‘...")
    saved_files = report.save_reports()
    report.log_console("âœ… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ!")
    report.log_console(f"   - JSON ìƒì„¸ ë³´ê³ ì„œ: {saved_files['json_report']}")
    report.log_console(f"   - TXT ìš”ì•½ ë³´ê³ ì„œ: {saved_files['txt_summary']}")

    logger.info("\n" + "=" * 60)
    logger.info("Task 3.6 í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    logger.info("=" * 60)

if __name__ == "__main__":
    asyncio.run(run_integrated_pipeline_tests())
